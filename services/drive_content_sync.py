"""
Google Drive ‚Üí lessons.json sync service.

Goal: allow editors to update course content in Google Drive without redeploying code.

Drive structure (recommended):
  <DRIVE_ROOT_FOLDER_ID>/
    day_00/
      lesson (Google Doc) OR lesson.txt OR lesson.html
      task  (Google Doc) OR task.txt  OR task.html
      meta.json (optional)
      media/ (optional subfolder) OR media files directly inside day_00
    day_01/
    ...

This service compiles all available days into a JSON compatible with existing LessonLoader,
and downloads media files to a persistent directory (Railway Volume) so CourseBot can send
them via FSInputFile when no Telegram file_id is present.
"""

from __future__ import annotations

import base64
import io
import json
import logging
import os
import re
import html
import shutil
from datetime import datetime, timezone
from html.parser import HTMLParser
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, List, Tuple

from core.config import Config

logger = logging.getLogger(__name__)


GOOGLE_DOC_MIME = "application/vnd.google-apps.document"
FOLDER_MIME = "application/vnd.google-apps.folder"


@dataclass
class SyncResult:
    days_synced: int
    lessons_path: str
    media_files_downloaded: int
    total_blocks: int  # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ (posts) –≤–æ –≤—Å–µ—Ö —É—Ä–æ–∫–∞—Ö
    total_media_files: int  # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ–¥–∏–∞—Ñ–∞–π–ª–æ–≤ (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö, –Ω–µ —Ç–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö)
    warnings: List[str]


class DriveContentSync:
    def __init__(self):
        self.enabled = str(getattr(Config, "DRIVE_CONTENT_ENABLED", "0")).strip() == "1"
        self.root_folder_id = (getattr(Config, "DRIVE_ROOT_FOLDER_ID", "") or "").strip()
        self.media_dir = (getattr(Config, "DRIVE_MEDIA_DIR", "data/content_media") or "data/content_media").strip()

    def _admin_ready(self) -> Tuple[bool, str]:
        if not self.enabled:
            return False, "DRIVE_CONTENT_ENABLED=0"
        if not self.root_folder_id and not (Config.DRIVE_MASTER_DOC_ID or "").strip():
            return False, "Set DRIVE_ROOT_FOLDER_ID (folders mode) or DRIVE_MASTER_DOC_ID (single-doc mode)"
        if not (Config.GOOGLE_SERVICE_ACCOUNT_JSON or Config.GOOGLE_SERVICE_ACCOUNT_JSON_B64):
            return False, "Google service account creds are missing (GOOGLE_SERVICE_ACCOUNT_JSON[_B64])"
        return True, "ok"

    @staticmethod
    def _extract_drive_file_ids(text: str) -> List[str]:
        """
        Extract Drive file IDs from common URL forms:
          - https://drive.google.com/file/d/<id>/view
          - https://drive.google.com/open?id=<id>
          - https://docs.google.com/document/d/<id>/edit
          - https://drive.google.com/uc?id=<id>
        """
        if not text:
            return []
        ids = set()
        patterns = [
            r"https?://drive\.google\.com/file/d/([a-zA-Z0-9_-]{10,})",
            r"https?://drive\.google\.com/open\?id=([a-zA-Z0-9_-]{10,})",
            r"https?://drive\.google\.com/uc\?id=([a-zA-Z0-9_-]{10,})",
            r"https?://docs\.google\.com/document/d/([a-zA-Z0-9_-]{10,})",
        ]
        for p in patterns:
            for m in re.findall(p, text):
                ids.add(m)
        return list(ids)
    
    @staticmethod
    def _find_drive_links_with_positions(text: str) -> List[Dict[str, str]]:
        """
        Find all Drive links in text with their positions and file/folder IDs.
        Returns list of dicts with 'url' (original from text), 'file_id' (or 'folder_id'), 'start', 'end', 'is_folder'.
        """
        if not text:
            return []
        
        links = []
        # Patterns to match various Google Drive URL formats
        # Important: we capture the FULL original URL from text for replacement
        patterns = [
            # Folder link: https://drive.google.com/drive/folders/FOLDER_ID?usp=drive_link
            (r"https?://drive\.google\.com/drive/folders/([a-zA-Z0-9_-]{10,})(?:/[^?\s]*)?(?:\?[^\s]*)?", lambda m: m.group(0), True),
            # Standard file link: https://drive.google.com/file/d/FILE_ID/view?usp=drive_link
            (r"https?://drive\.google\.com/file/d/([a-zA-Z0-9_-]{10,})(?:/[^?\s]*)?(?:\?[^\s]*)?", lambda m: m.group(0), False),
            # Open link: https://drive.google.com/open?id=FILE_ID
            (r"https?://drive\.google\.com/open\?id=([a-zA-Z0-9_-]{10,})(?:&[^\s]*)?", lambda m: m.group(0), False),
            # UC link: https://drive.google.com/uc?id=FILE_ID
            (r"https?://drive\.google\.com/uc\?id=([a-zA-Z0-9_-]{10,})(?:&[^\s]*)?", lambda m: m.group(0), False),
            # Document link: https://docs.google.com/document/d/FILE_ID/...
            (r"https?://docs\.google\.com/document/d/([a-zA-Z0-9_-]{10,})(?:/[^\s]*)?", lambda m: m.group(0), False),
        ]
        
        for pattern, url_extractor, is_folder in patterns:
            for match in re.finditer(pattern, text):
                item_id = match.group(1)
                # Use the FULL original URL from text (including /view?usp=drive_link etc.)
                original_url = url_extractor(match)
                links.append({
                    "url": original_url,  # Original URL from text for exact replacement
                    "file_id": item_id,  # For folders, this is actually folder_id
                    "folder_id": item_id if is_folder else None,  # Explicit folder_id field
                    "is_folder": is_folder,
                    "start": match.start(),
                    "end": match.end()
                })
        
        # IMPORTANT: do NOT dedupe occurrences here. The same Drive URL can appear multiple times
        # and must be replaced in every position to preserve exact placement. Downloading is
        # deduped later by file_id/folder_id.
        return links

    @staticmethod
    def _split_lesson_into_posts(lesson_text: str, max_length: int = 4000) -> List[str]:
        """
        –†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç —É—Ä–æ–∫–∞ –Ω–∞ –ø–æ—Å—Ç—ã —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
        
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
        1. –†—É—á–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã: [POST], [–î–û–ü–û–õ–ù–ï–ù–ò–ï], [BLOCK], [–ë–õ–û–ö] –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
        2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–µ (>4000 —Å–∏–º–≤–æ–ª–æ–≤) –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö –∞–±–∑–∞—Ü–µ–≤
        
        –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–ø—Ä–æ–±–µ–ª—ã, –æ—Ç—Å—Ç—É–ø—ã, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, —ç–º–æ–¥–∑–∏)
        –í–ê–ñ–ù–û: –ú–µ–¥–∏–∞-–º–∞—Ä–∫–µ—Ä—ã [MEDIA_...] –ù–ï —è–≤–ª—è—é—Ç—Å—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ –ø–æ—Å—Ç–æ–≤
        
        Args:
            lesson_text: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —É—Ä–æ–∫–∞
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 4000, –ª–∏–º–∏—Ç Telegram 4096)
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å—Ç–æ–≤
        """
        if not lesson_text or not lesson_text.strip():
            return [""]
        
        # –®–∞–≥ 1: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä—É—á–Ω—ã–º –º–∞—Ä–∫–µ—Ä–∞–º
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º: [POST], [–î–û–ü–û–õ–ù–ï–ù–ò–ï], [BLOCK], [–ë–õ–û–ö] –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
        # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        lines = lesson_text.split('\n')
        posts = []
        current_post_lines = []
        
        for line in lines:
            line_stripped = line.strip()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –º–µ–¥–∏–∞-–º–∞—Ä–∫–µ—Ä–æ–º (–Ω–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø–æ—Å—Ç–æ–≤)
            is_media_marker = re.match(r'^\s*\[MEDIA_[a-zA-Z0-9_-]+\]\s*$', line)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –º–∞—Ä–∫–µ—Ä–æ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º: [POST], [–î–û–ü–û–õ–ù–ï–ù–ò–ï], [BLOCK], [–ë–õ–û–ö] –∏ –∏—Ö –≤–∞—Ä–∏–∞—Ü–∏–∏
            is_post_marker = False
            if not is_media_marker:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞—Ä–∫–µ—Ä—ã –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
                post_marker_pattern = r'^\s*\[(POST|–î–û–ü–û–õ–ù–ï–ù–ò–ï|BLOCK|–ë–õ–û–ö|POST\d*|–î–û–ü–û–õ–ù–ï–ù–ò–ï\d*)\]\s*$'
                if re.match(post_marker_pattern, line, re.IGNORECASE):
                    is_post_marker = True
            
            if is_post_marker:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –ø–æ—Å—Ç, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π
                if current_post_lines:
                    post_text = '\n'.join(current_post_lines)
                    # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
                    post_text = post_text.rstrip()
                    if post_text:
                        posts.append(post_text)
                    current_post_lines = []
                # –ú–∞—Ä–∫–µ—Ä –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø–æ—Å—Ç
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –≤ —Ç–µ–∫—É—â–∏–π –ø–æ—Å—Ç (—Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)
                current_post_lines.append(line)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ—Å—Ç, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        if current_post_lines:
            post_text = '\n'.join(current_post_lines)
            post_text = post_text.rstrip()
            if post_text:
                posts.append(post_text)
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –º–∞—Ä–∫–µ—Ä—ã –∏ —Ä–∞–∑–¥–µ–ª–∏–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å—Ç—ã
        if len(posts) > 1:
            return posts
        
        # –®–∞–≥ 2: –ï—Å–ª–∏ —Ä—É—á–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ –Ω–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∏ —Ä–∞–∑–¥–µ–ª—è–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        lesson_text_stripped = lesson_text.rstrip()
        if len(lesson_text_stripped) <= max_length:
            return [lesson_text_stripped] if lesson_text_stripped else [""]
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∞–±–∑–∞—Ü–∞–º, —Å—Ç–∞—Ä–∞—è—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Å—Ç—ã –ø–æ–¥ max_length
        posts = []
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã (–¥–≤–æ–π–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫), —Å–æ—Ö—Ä–∞–Ω—è—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        paragraphs = lesson_text.split('\n\n')
        current_post_parts = []
        current_length = 0
        
        for para in paragraphs:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∞–±–∑–∞—Ü —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            para_original = para
            para_stripped = para.rstrip()
            
            if not para_stripped:
                # –ü—É—Å—Ç–æ–π –∞–±–∑–∞—Ü - –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                if current_post_parts:
                    current_post_parts.append("")
                continue
            
            para_length = len(para_stripped)
            
            # –ï—Å–ª–∏ –æ–¥–∏–Ω –∞–±–∑–∞—Ü –ø—Ä–µ–≤—ã—à–∞–µ—Ç max_length, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ –ø–æ —Å—Ç—Ä–æ–∫–∞–º
            if para_length > max_length:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –ø–æ—Å—Ç, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                if current_post_parts:
                    post_text = '\n\n'.join(current_post_parts)
                    post_text = post_text.rstrip()
                    if post_text:
                        posts.append(post_text)
                    current_post_parts = []
                    current_length = 0
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π –∞–±–∑–∞—Ü –ø–æ —Å—Ç—Ä–æ–∫–∞–º
                para_lines = para.split('\n')
                current_line_parts = []
                current_line_length = 0
                
                for line in para_lines:
                    line_stripped = line.rstrip()
                    if not line_stripped:
                        # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ - –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                        if current_line_parts:
                            current_line_parts.append("")
                        continue
                    
                    line_length = len(line_stripped)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ max_length
                    if current_line_parts and current_line_length + line_length + 2 > max_length:  # +2 –¥–ª—è \n\n
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –ø–æ—Å—Ç
                        if current_line_parts:
                            post_text = '\n'.join(current_line_parts)
                            post_text = post_text.rstrip()
                            if post_text:
                                posts.append(post_text)
                            current_line_parts = []
                            current_line_length = 0
                    
                    current_line_parts.append(line)
                    current_line_length += len(line) + 1  # +1 –¥–ª—è \n
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ—Å—Ç
                if current_line_parts:
                    post_text = '\n'.join(current_line_parts)
                    post_text = post_text.rstrip()
                    if post_text:
                        posts.append(post_text)
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–±–∑–∞—Ü–∞ max_length
                if current_post_parts and current_length + para_length + 2 > max_length:  # +2 –¥–ª—è \n\n
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –ø–æ—Å—Ç
                    post_text = '\n\n'.join(current_post_parts)
                    post_text = post_text.rstrip()
                    if post_text:
                        posts.append(post_text)
                    current_post_parts = []
                    current_length = 0
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∞–±–∑–∞—Ü –≤ —Ç–µ–∫—É—â–∏–π –ø–æ—Å—Ç (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)
                current_post_parts.append(para_original)
                current_length += len(para_original) + 2  # +2 –¥–ª—è \n\n
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–π—Å—è –ø–æ—Å—Ç
        if current_post_parts:
            post_text = '\n\n'.join(current_post_parts)
            post_text = post_text.rstrip()
            if post_text:
                posts.append(post_text)
        
        return posts if posts else [lesson_text_stripped] if lesson_text_stripped else [""]

    def _split_master_doc(self, text: str) -> Dict[int, Dict[str, str]]:
        """
        –†–∞–∑–¥–µ–ª—è–µ—Ç –º–∞—Å—Ç–µ—Ä-–¥–æ–∫—É–º–µ–Ω—Ç (plain text export) –Ω–∞ –±–ª–æ–∫–∏ –ø–æ –¥–Ω—è–º.
        
        –û–∂–∏–¥–∞–µ–º—ã–µ –º–∞—Ä–∫–µ—Ä—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ (—Ä—É—Å—Å–∫–∏–π –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π):
          - "–î–µ–Ω—å 0" ... "–î–µ–Ω—å 30"
          - "Day 0" ... "Day 30"
        
        –í–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ –¥–Ω—è:
          - –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞: "–ó–∞–≥–æ–ª–æ–≤–æ–∫: ..." –∏–ª–∏ "Title: ..."
          - –ó–∞–¥–∞–Ω–∏–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å–æ —Å—Ç—Ä–æ–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–µ–π—Å—è —Å "–ó–∞–¥–∞–Ω–∏–µ:" / "Task:"
          - –í–≤–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: "Intro:" –∏–ª–∏ "–í–≤–µ–¥–µ–Ω–∏–µ:" –∏–ª–∏ "–í–≤–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:"
          - –û–±–æ –º–Ω–µ: "–û–±–æ –º–Ω–µ:" –∏–ª–∏ "About me:" –∏–ª–∏ "About_me:"
        
        –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–ø—Ä–æ–±–µ–ª—ã, –æ—Ç—Å—Ç—É–ø—ã, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, —ç–º–æ–¥–∑–∏)
        """
        blocks: Dict[int, List[str]] = {}
        current_day: Optional[int] = None

        lines = (text or "").splitlines()
        day_header_re = re.compile(r"^\s*(?:–î–µ–Ω—å|Day)\s+(\d{1,2})\s*(?::\s*(.*))?$", re.IGNORECASE)

        for line in lines:
            m = day_header_re.match(line)
            if m:
                day = int(m.group(1))
                if 0 <= day <= 30:
                    current_day = day
                    blocks.setdefault(day, [])
                    # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–º–µ–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –¥–≤–æ–µ—Ç–æ—á–∏—è, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –∫–∞–∫ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
                    title_hint = (m.group(2) or "").strip()
                    if title_hint:
                        blocks[day].append(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title_hint}")
                    continue
            if current_day is not None:
                blocks[current_day].append(line)

        out: Dict[int, Dict[str, str]] = {}
        for day, bl in blocks.items():
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            raw = "\n".join(bl)
            if not raw.strip():
                continue

            title = ""
            lesson = raw
            task = ""
            intro_text = ""
            about_me_text = ""

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            for ln in bl[:10]:
                mm = re.match(r"^\s*(?:–ó–∞–≥–æ–ª–æ–≤–æ–∫|Title)\s*:\s*(.+)\s*$", ln, re.IGNORECASE)
                if mm:
                    title = mm.group(1).strip()
                    break

            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Å–µ–∫—Ü–∏–∏: task, intro_text, about_me_text
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –æ–±–∞ —Ñ–æ—Ä–º–∞—Ç–∞:
            #   "–ó–∞–¥–∞–Ω–∏–µ:" (–Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ)
            #   "–ó–∞–¥–∞–Ω–∏–µ: —Ç–µ–∫—Å—Ç –∑–∞–¥–∞–Ω–∏—è" (—Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–π –∂–µ —Å—Ç—Ä–æ–∫–µ)
            task_re = re.compile(r"^\s*(?:–ó–∞–¥–∞–Ω–∏–µ|Task)\s*:\s*(.*)$", re.IGNORECASE)
            intro_re = re.compile(r"^\s*(?:Intro|–í–≤–µ–¥–µ–Ω–∏–µ|–í–≤–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç)\s*:\s*(.*)$", re.IGNORECASE)
            about_me_re = re.compile(r"^\s*(?:–û–±–æ –º–Ω–µ|About me|About_me)\s*:\s*(.*)$", re.IGNORECASE)
            
            parts_lesson: List[str] = []
            parts_task: List[str] = []
            parts_intro: List[str] = []
            parts_about_me: List[str] = []
            
            in_task = False
            in_intro = False
            in_about_me = False
            
            for ln in bl:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞—Ä–∫–µ—Ä –∑–∞–¥–∞–Ω–∏—è
                m_task = task_re.match(ln)
                if m_task:
                    in_task = True
                    in_intro = False
                    in_about_me = False
                    task_text_on_line = (m_task.group(1) or "").strip()
                    if task_text_on_line:
                        parts_task.append(task_text_on_line)
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞—Ä–∫–µ—Ä –≤–≤–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                m_intro = intro_re.match(ln)
                if m_intro:
                    in_intro = True
                    in_task = False
                    in_about_me = False
                    intro_text_on_line = (m_intro.group(1) or "").strip()
                    if intro_text_on_line:
                        parts_intro.append(intro_text_on_line)
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞—Ä–∫–µ—Ä "–û–±–æ –º–Ω–µ"
                m_about_me = about_me_re.match(ln)
                if m_about_me:
                    in_about_me = True
                    in_task = False
                    in_intro = False
                    about_me_text_on_line = (m_about_me.group(1) or "").strip()
                    if about_me_text_on_line:
                        parts_about_me.append(about_me_text_on_line)
                    continue
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —Å–µ–∫—Ü–∏—é
                if in_task:
                    parts_task.append(ln)
                elif in_intro:
                    parts_intro.append(ln)
                elif in_about_me:
                    parts_about_me.append(ln)
                else:
                    parts_lesson.append(ln)
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç —Å–µ–∫—Ü–∏–π, —Å–æ—Ö—Ä–∞–Ω—è—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            lesson = "\n".join(parts_lesson).rstrip()
            task = "\n".join(parts_task).rstrip()
            intro_text = "\n".join(parts_intro).rstrip()
            about_me_text = "\n".join(parts_about_me).rstrip()

            # –†–∞–∑–¥–µ–ª—è–µ–º —É—Ä–æ–∫ –Ω–∞ –ø–æ—Å—Ç—ã:
            # 1. –ü–æ —Ä—É—á–Ω—ã–º –º–∞—Ä–∫–µ—Ä–∞–º: [POST], [–î–û–ü–û–õ–ù–ï–ù–ò–ï] –∏ —Ç.–¥.
            # 2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ –¥–ª–∏–Ω–µ (>4000 —Å–∏–º–≤–æ–ª–æ–≤)
            lesson_posts = DriveContentSync._split_lesson_into_posts(lesson)
            
            # –ï—Å–ª–∏ —É—Ä–æ–∫ —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Å–ø–∏—Å–æ–∫; –∏–Ω–∞—á–µ –∫–∞–∫ —Å—Ç—Ä–æ–∫—É (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
            lesson_data = {
                "title": title, 
                "lesson": lesson_posts if len(lesson_posts) > 1 else (lesson_posts[0] if lesson_posts else ""), 
                "task": task
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º intro_text –∏ about_me_text, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã
            if intro_text:
                lesson_data["intro_text"] = intro_text
            if about_me_text:
                lesson_data["about_me_text"] = about_me_text
            
            out[day] = lesson_data

        return out
    
    def _sync_from_master_doc(self, drive, warnings: List[str]) -> Tuple[Dict[str, Any], int, int, int]:
        master_id = (Config.DRIVE_MASTER_DOC_ID or "").strip()
        if not master_id:
            raise RuntimeError("DRIVE_MASTER_DOC_ID is empty")

        # –°–∫–∞—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –º–∞—Å—Ç–µ—Ä-–¥–æ–∫—É–º–µ–Ω—Ç–∞
        master_text = self._download_text_file(drive, master_id, GOOGLE_DOC_MIME)
        master_text, w = self._sanitize_telegram_html(master_text or "")
        if w:
            warnings.extend([f"master_doc: {x}" for x in w])

        day_map = self._split_master_doc(master_text)
        if not day_map:
            raise RuntimeError("Could not find any '–î–µ–Ω—å N' sections in master doc")

        project_root = Path.cwd()
        media_root = (project_root / self.media_dir).resolve()
        media_downloaded = 0

        compiled: Dict[str, Any] = {}
        total_blocks = 0
        total_media_files = 0
        for day, data in sorted(day_map.items(), key=lambda x: x[0]):
            title = (data.get("title") or "").strip() or f"–î–µ–Ω—å {day}"
            lesson_raw = data.get("lesson") or ""
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –º–Ω–æ–≥–æ-–ø–æ—Å—Ç–æ–≤—ã–µ —É—Ä–æ–∫–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–µ –∫–∞–∫ —Å–ø–∏—Å–æ–∫
            # –í–ê–ñ–ù–û: –ï—Å–ª–∏ —É—Ä–æ–∫ —É–∂–µ —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –ø–æ—Å—Ç—ã (—á–µ—Ä–µ–∑ –º–∞—Ä–∫–µ—Ä—ã [POST]), —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ—Å—Ç–æ–≤
            lesson_was_split = isinstance(lesson_raw, list)
            if lesson_was_split:
                lesson_posts_list = [str(x).rstrip() for x in lesson_raw if str(x).strip()]
                lesson_text = "\n\n".join(lesson_posts_list)
            else:
                lesson_text = str(lesson_raw).rstrip()
                lesson_posts_list = None
            task_text = (data.get("task") or "").rstrip()
            intro_text = (data.get("intro_text") or "").rstrip()
            about_me_text = (data.get("about_me_text") or "").rstrip()

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–µ–¥–∏–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å Drive, —É–ø–æ–º—è–Ω—É—Ç—ã–µ –≤ —Ç–µ–∫—Å—Ç–µ/–∑–∞–¥–∞–Ω–∏–∏
            # –ó–∞–º–µ–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–∞—Ä–∫–µ—Ä—ã –∏ —Å–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã
            media_items: List[Dict[str, Any]] = []
            media_markers: Dict[str, Dict[str, Any]] = {}  # marker_id -> media_info
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ Drive —Å—Å—ã–ª–∫–∏ –≤ lesson, task, intro_text, –∏ about_me_text
            # –í–ê–ñ–ù–û: –í —Ä–µ–∂–∏–º–µ master doc intro_text –∏ about_me_text –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∏–∑ —Å–∞–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —É—Ä–æ–∫–∞
            # –û–Ω–∏ —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã –≤ _split_master_doc, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –∑–¥–µ—Å—å
            combined_text = lesson_text + "\n" + task_text
            if intro_text:
                combined_text += "\n" + intro_text
            if about_me_text:
                combined_text += "\n" + about_me_text
            
            drive_links = self._find_drive_links_with_positions(combined_text)
            
            logger.info(f"   üìé Day {day}: Found {len(drive_links)} Drive links in text")
            logger.info(f"   üìé Day {day}: Text lengths - lesson: {len(lesson_text)}, task: {len(task_text)}, intro: {len(intro_text)}, about_me: {len(about_me_text)}")
            if drive_links:
                for idx, link in enumerate(drive_links, 1):
                    logger.info(f"   üìé   [{idx}/{len(drive_links)}] Link: {link['url'][:80]}... (file_id: {link['file_id']}, is_folder: {link.get('is_folder', False)}, position: {link.get('start', '?')}-{link.get('end', '?')})")
            else:
                logger.warning(f"   ‚ö†Ô∏è Day {day}: No Drive links found in text! This may indicate a problem with link detection.")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –ø—Ä–∏ –∑–∞–º–µ–Ω–µ
            drive_links.sort(key=lambda x: x["start"], reverse=True)
            
            processed_links = 0
            skipped_links = 0
            error_links = 0
            
            # –í–ê–ñ–ù–û: –ó–∞–º–µ–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ, —Å–æ—Ö—Ä–∞–Ω—è—è —Ç–æ—á–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫, —á—Ç–æ–±—ã –ø–æ–∑–∏—Ü–∏–∏ –Ω–µ —Å–¥–≤–∏–≥–∞–ª–∏—Å—å –ø—Ä–∏ –∑–∞–º–µ–Ω–µ
            for link_info in drive_links:
                fid = link_info["file_id"]
                link_url = link_info["url"]
                is_folder = link_info.get("is_folder", False)
                
                try:
                    if is_folder:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–ø–∫—É: –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∫–∞–∫ –º–µ–¥–∏–∞
                        folder_id = link_info.get("folder_id") or fid
                        logger.info(f"   üìÅ Processing Drive folder: {link_url[:60]}... (folder_id: {folder_id})")
                        
                        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ
                        folder_files = self._list_children(drive, folder_id)
                        logger.info(f"   üìÅ   Found {len(folder_files)} items in folder")
                        
                        if not folder_files:
                            logger.warning(f"   ‚ö†Ô∏è Folder {folder_id} is empty or inaccessible")
                            skipped_links += 1
                            continue
                        
                        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –º–∞—Ä–∫–µ—Ä—ã –¥–ª—è —Ñ–∞–π–ª–æ–≤ –≤ —ç—Ç–æ–π –ø–∞–ø–∫–µ
                        folder_markers = []
                        folder_media_count = 0
                        
                        for folder_file in folder_files:
                            file_id = folder_file.get("id")
                            file_name = folder_file.get("name", f"file_{file_id}").strip()
                            file_mime = (folder_file.get("mimeType") or "").lower()
                            
                            logger.info(f"   üìÅ   Processing folder item: {file_name} (MIME: {file_mime})")
                            
                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –º–µ–¥–∏–∞-—Ñ–∞–π–ª—ã (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è/–≤–∏–¥–µ–æ)
                            if file_mime.startswith("image/"):
                                media_type = "photo"
                            elif file_mime.startswith("video/"):
                                media_type = "video"
                            else:
                                logger.info(f"   üìÅ   Skipping non-media file in folder: {file_name} (MIME: {file_mime})")
                                continue
                            
                            safe_name = re.sub(r"[^a-zA-Z0-9._-]+", "_", file_name)
                            dest = media_root / f"day_{day:02d}" / safe_name
                            
                            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                            dest.parent.mkdir(parents=True, exist_ok=True)
                            
                            should_skip = self._should_skip_download(dest, folder_file.get("size"), folder_file.get("modifiedTime"))
                            file_exists = dest.exists()
                            logger.info(f"   üìÅ     Destination: {dest}, File exists: {file_exists}, Should skip: {should_skip}")
                            
                            if not should_skip or not file_exists:
                                if not file_exists:
                                    logger.info(f"   üìÅ     File doesn't exist, downloading: {file_name} -> {dest}")
                                else:
                                    logger.info(f"   üìÅ     File outdated or size mismatch, re-downloading: {file_name} -> {dest}")
                                self._download_binary_file(drive, file_id, dest)
                                media_downloaded += 1
                                logger.info(f"   ‚úÖ Downloaded media file from folder: {file_name} (total downloaded: {media_downloaded})")
                            else:
                                logger.info(f"   üìÅ     File already exists and up-to-date, skipping download: {dest}")
                            
                            processed_links += 1
                            rel_path = str(dest.relative_to(project_root)).replace("\\", "/")
                            
                            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª –ª–∏ —É–∂–µ —Å–æ–∑–¥–∞–Ω –º–∞—Ä–∫–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ file_id
                            # –ï—Å–ª–∏ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Ñ–∞–π–ª –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –ø–∞–ø–∫–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω –º–∞—Ä–∫–µ—Ä
                            marker_id = None
                            for existing_marker_id, existing_info in media_markers.items():
                                if existing_info.get("file_id") == file_id and existing_info.get("path") == rel_path:
                                    marker_id = existing_marker_id
                                    logger.info(f"   ‚ôªÔ∏è Reusing existing marker: [{marker_id}] for file {file_name} (file_id: {file_id})")
                                    break
                            
                            # –ï—Å–ª–∏ –º–∞—Ä–∫–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
                            if not marker_id:
                                marker_id = f"MEDIA_{file_id}_{len(media_markers)}"
                                media_markers[marker_id] = {
                                    "type": media_type,
                                    "path": rel_path,
                                    "file_id": file_id,
                                    "name": file_name
                                }
                                logger.info(f"   ‚úÖ Created new media marker: [{marker_id}] for file {file_name} (path: {rel_path})")
                            
                            folder_markers.append(marker_id)
                            folder_media_count += 1
                            
                            media_items.append({"type": media_type, "path": rel_path, "marker_id": marker_id})
                        
                        # –ó–∞–º–µ–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –ø–∞–ø–∫—É –≤—Å–µ–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É –∏–ª–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)
                        if folder_markers:
                            # –ó–∞–º–µ–Ω—è–µ–º URL –ø–∞–ø–∫–∏ –≤—Å–µ–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏, –ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É
                            markers_text = "\n".join([f"[{m}]" for m in folder_markers])
                            replaced_in_lesson = False
                            replaced_in_task = False
                            replaced_in_intro = False
                            replaced_in_about_me = False
                            
                            # –í–ê–ñ–ù–û: –ó–∞–º–µ–Ω—è–µ–º —Ç–æ—á–Ω—É—é —Å—Å—ã–ª–∫—É –≤ –∫–∞–∂–¥–æ–º –ø–æ–ª–µ —Ç–µ–∫—Å—Ç–∞
                            if link_url in lesson_text:
                                lesson_text = lesson_text.replace(link_url, markers_text)
                                replaced_in_lesson = True
                                logger.info(f"   ‚úÖ Replaced Drive folder link in lesson_text with {len(folder_markers)} markers")
                            if link_url in task_text:
                                task_text = task_text.replace(link_url, markers_text)
                                replaced_in_task = True
                                logger.info(f"   ‚úÖ Replaced Drive folder link in task_text with {len(folder_markers)} markers")
                            if intro_text and link_url in intro_text:
                                intro_text = intro_text.replace(link_url, markers_text)
                                replaced_in_intro = True
                                logger.info(f"   ‚úÖ Replaced Drive folder link in intro_text with {len(folder_markers)} markers")
                            if about_me_text and link_url in about_me_text:
                                about_me_text = about_me_text.replace(link_url, markers_text)
                                replaced_in_about_me = True
                                logger.info(f"   ‚úÖ Replaced Drive folder link in about_me_text with {len(folder_markers)} markers")
                            
                            if not replaced_in_lesson and not replaced_in_task and not replaced_in_intro and not replaced_in_about_me:
                                logger.warning(f"   ‚ö†Ô∏è Drive folder link not found in any text field: {link_url[:60]}...")
                            
                            logger.info(f"   üìÅ Folder processed: {folder_media_count} media files, {len(folder_markers)} markers created")
                        else:
                            logger.warning(f"   ‚ö†Ô∏è No media files found in folder {folder_id}")
                            skipped_links += 1
                    else:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–π —Ñ–∞–π–ª (—Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞)
                        logger.info(f"   üìé Processing Drive link: {link_url[:60]}... (file_id: {fid})")
                        
                        meta = drive.files().get(fileId=fid, fields="id,name,mimeType,modifiedTime,size").execute()
                        mt = (meta.get("mimeType") or "").lower()
                        name = (meta.get("name") or f"file_{fid}").strip()
                        
                        logger.info(f"   üìé   File name: {name}, MIME type: {mt}")
                        
                        if mt.startswith("image/"):
                            media_type = "photo"
                        elif mt.startswith("video/"):
                            media_type = "video"
                        else:
                            logger.info(f"   üìé   Skipping non-media file: {name} (MIME: {mt})")
                            skipped_links += 1
                            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ-–º–µ–¥–∏–∞ —Ñ–∞–π–ª—ã
                        
                        safe_name = re.sub(r"[^a-zA-Z0-9._-]+", "_", name)
                        dest = media_root / f"day_{day:02d}" / safe_name
                        
                        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                        dest.parent.mkdir(parents=True, exist_ok=True)
                        
                        should_skip = self._should_skip_download(dest, meta.get("size"), meta.get("modifiedTime"))
                        file_exists = dest.exists()
                        logger.info(f"   üìé   Destination: {dest}, File exists: {file_exists}, Should skip: {should_skip}")
                        
                        if not should_skip or not file_exists:
                            if not file_exists:
                                logger.info(f"   üìé   File doesn't exist, downloading: {name} -> {dest}")
                            else:
                                logger.info(f"   üìé   File outdated or size mismatch, re-downloading: {name} -> {dest}")
                            self._download_binary_file(drive, fid, dest)
                            media_downloaded += 1
                            logger.info(f"   ‚úÖ Downloaded media file: {name} (total downloaded: {media_downloaded})")
                        else:
                            logger.info(f"   üìé   File already exists and up-to-date, skipping download: {dest}")
                            # –°—á–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã –∫–∞–∫ "–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ" –¥–ª—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏
                            skipped_links += 1
                        
                        processed_links += 1
                        rel_path = str(dest.relative_to(project_root)).replace("\\", "/")
                        
                        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª –ª–∏ —É–∂–µ —Å–æ–∑–¥–∞–Ω –º–∞—Ä–∫–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ file_id
                        # –ï—Å–ª–∏ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Ñ–∞–π–ª –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω –º–∞—Ä–∫–µ—Ä
                        marker_id = None
                        for existing_marker_id, existing_info in media_markers.items():
                            if existing_info.get("file_id") == fid and existing_info.get("path") == rel_path:
                                marker_id = existing_marker_id
                                logger.info(f"   ‚ôªÔ∏è Reusing existing marker: [{marker_id}] for file {name} (file_id: {fid})")
                                break
                        
                        # –ï—Å–ª–∏ –º–∞—Ä–∫–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
                        if not marker_id:
                            marker_id = f"MEDIA_{fid}_{len(media_markers)}"
                            media_markers[marker_id] = {
                                "type": media_type,
                                "path": rel_path,
                                "file_id": fid,
                                "name": name
                            }
                            logger.info(f"   ‚úÖ Created new media marker: [{marker_id}] for file {name} (path: {rel_path})")
                        
                        # –ó–∞–º–µ–Ω—è–µ–º —Å—Å—ã–ª–∫—É –≤ —Ç–µ–∫—Å—Ç–µ –Ω–∞ –º–∞—Ä–∫–µ—Ä
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π URL –∏–∑ —Ç–µ–∫—Å—Ç–∞ (link_url) –¥–ª—è —Ç–æ—á–Ω–æ–π –∑–∞–º–µ–Ω—ã
                        marker_placeholder = f"[{marker_id}]"
                        # –ó–∞–º–µ–Ω—è–µ–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è —Å—Å—ã–ª–∫–∏ URL –≤ –æ–±–æ–∏—Ö —Ç–µ–∫—Å—Ç–∞—Ö, intro_text, –∏ about_me_text
                        replaced_in_lesson = False
                        replaced_in_task = False
                        replaced_in_intro = False
                        replaced_in_about_me = False
                        
                        # –í–ê–ñ–ù–û: –ó–∞–º–µ–Ω—è–µ–º —Ç–æ—á–Ω—É—é —Å—Å—ã–ª–∫—É –≤ –∫–∞–∂–¥–æ–º –ø–æ–ª–µ —Ç–µ–∫—Å—Ç–∞
                        if link_url in lesson_text:
                            lesson_text = lesson_text.replace(link_url, marker_placeholder)
                            replaced_in_lesson = True
                            logger.info(f"   ‚úÖ Replaced Drive link in lesson_text: {link_url[:60]}... -> [{marker_id}]")
                        if link_url in task_text:
                            task_text = task_text.replace(link_url, marker_placeholder)
                            replaced_in_task = True
                            logger.info(f"   ‚úÖ Replaced Drive link in task_text: {link_url[:60]}... -> [{marker_id}]")
                        if intro_text and link_url in intro_text:
                            intro_text = intro_text.replace(link_url, marker_placeholder)
                            replaced_in_intro = True
                            logger.info(f"   ‚úÖ Replaced Drive link in intro_text: {link_url[:60]}... -> [{marker_id}]")
                        if about_me_text and link_url in about_me_text:
                            about_me_text = about_me_text.replace(link_url, marker_placeholder)
                            replaced_in_about_me = True
                            logger.info(f"   ‚úÖ Replaced Drive link in about_me_text: {link_url[:60]}... -> [{marker_id}]")
                        
                        if not replaced_in_lesson and not replaced_in_task and not replaced_in_intro and not replaced_in_about_me:
                            logger.warning(f"   ‚ö†Ô∏è Drive link not found in any text field: {link_url[:60]}...")
                            logger.warning(f"   ‚ö†Ô∏è This may indicate the link format changed or was already replaced")
                        
                        media_items.append({"type": media_type, "path": rel_path, "marker_id": marker_id})
                except Exception as e:
                    error_links += 1
                    error_msg = f"day {day}: failed to download linked media ({fid}): {e}"
                    logger.error(f"   ‚ùå {error_msg}")
                    warnings.append(error_msg)
            
            if drive_links:
                logger.info(f"   üìé Day {day} summary: {processed_links} processed, {skipped_links} skipped, {error_links} errors, {media_downloaded} downloaded")

            # –†–∞–∑–¥–µ–ª—è–µ–º —É—Ä–æ–∫ –Ω–∞ –ø–æ—Å—Ç—ã –ø–æ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–º —Å–∫–æ–±–∫–∞–º (–µ—Å–ª–∏ –µ—â–µ –Ω–µ —Ä–∞–∑–¥–µ–ª–µ–Ω)
            # –í–ê–ñ–ù–û: –ï—Å–ª–∏ —É—Ä–æ–∫ —É–∂–µ –±—ã–ª —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –ø–æ—Å—Ç—ã –≤ _split_master_doc, 
            # –Ω—É–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –µ–≥–æ —Å–Ω–æ–≤–∞ –ø–æ—Å–ª–µ –∑–∞–º–µ–Ω—ã —Å—Å—ã–ª–æ–∫ –Ω–∞ –º–∞—Ä–∫–µ—Ä—ã,
            # —á—Ç–æ–±—ã –º–∞—Ä–∫–µ—Ä—ã –º–µ–¥–∏–∞ –æ—Å—Ç–∞–ª–∏—Å—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–∞—Ö
            lesson_posts = DriveContentSync._split_lesson_into_posts(lesson_text)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –±–ª–æ–∫–∞—Ö –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            logger.info(f"   üì¶ Day {day}: Split into {len(lesson_posts)} blocks")
            for i, post in enumerate(lesson_posts, 1):
                post_preview = post[:100].replace('\n', ' ') if post else "(empty)"
                logger.debug(f"   üì¶   Block {i}/{len(lesson_posts)}: {len(post)} chars, preview: {post_preview}...")
            
            # –ï—Å–ª–∏ —É—Ä–æ–∫ –±—ã–ª —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –ø–æ—Å—Ç—ã –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ, —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞
            if lesson_was_split and lesson_posts_list:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ —Å–æ–≤–ø–∞–¥–∞–µ—Ç (–∏–ª–∏ –±–ª–∏–∑–∫–æ)
                # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –º–∞—Ä–∫–µ—Ä—ã [POST] –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
                if len(lesson_posts) < len(lesson_posts_list):
                    logger.warning(f"   ‚ö†Ô∏è Day {day}: Number of posts changed after link replacement ({len(lesson_posts_list)} -> {len(lesson_posts)}). This may indicate a problem with [POST] marker processing.")
                elif len(lesson_posts) > len(lesson_posts_list):
                    logger.info(f"   üìé Day {day}: Number of posts increased after link replacement ({len(lesson_posts_list)} -> {len(lesson_posts)}). This is normal if new [POST] markers were added.")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –±–ª–æ–∫–æ–≤: –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ—Ç –ø—É—Å—Ç—ã—Ö –±–ª–æ–∫–æ–≤
            valid_lesson_posts = [post for post in lesson_posts if post and post.strip()]
            if len(valid_lesson_posts) != len(lesson_posts):
                empty_count = len(lesson_posts) - len(valid_lesson_posts)
                logger.warning(f"   ‚ö†Ô∏è Day {day}: Found {empty_count} empty blocks, removing them")
                lesson_posts = valid_lesson_posts
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è
            original_length = len(lesson_text)
            saved_length = sum(len(post) for post in lesson_posts)
            if saved_length < original_length * 0.95:  # –î–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–±–æ–ª—å—à—É—é –ø–æ—Ç–µ—Ä—é –ø—Ä–∏ rstrip()
                logger.warning(f"   ‚ö†Ô∏è Day {day}: Possible text loss detected! Original: {original_length} chars, Saved: {saved_length} chars")
            else:
                logger.debug(f"   ‚úÖ Day {day}: Text integrity check passed (original: {original_length}, saved: {saved_length} chars)")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç: —Å–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤, –µ—Å–ª–∏ –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ, –∏–Ω–∞—á–µ —Å—Ç—Ä–æ–∫–∞
            text_to_save = lesson_posts if len(lesson_posts) > 1 else (lesson_posts[0] if lesson_posts else "")
            if isinstance(text_to_save, list):
                logger.info(f"   ‚úÖ Day {day}: Saving {len(text_to_save)} blocks as list (total {saved_length} chars)")
            else:
                logger.info(f"   ‚úÖ Day {day}: Saving single block as string ({len(text_to_save) if text_to_save else 0} chars)")
            
            entry: Dict[str, Any] = {
                "day_number": day,
                "title": title,
                "text": text_to_save,
                "task": task_text,
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º intro_text –∏ about_me_text, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏–∑ –º–∞—Å—Ç–µ—Ä-–¥–æ–∫—É–º–µ–Ω—Ç–∞
            if intro_text:
                entry["intro_text"] = intro_text
            if about_me_text:
                entry["about_me_text"] = about_me_text
            
            if media_items:
                entry["media"] = media_items
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞—Ä–∫–µ—Ä—ã –º–µ–¥–∏–∞ –¥–ª—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –≤—Å—Ç–∞–≤–∫–∏
            # –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞—Ä–∫–µ—Ä—ã, –µ—Å–ª–∏ –æ–Ω–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç, –¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∞–π–ª—ã –Ω–µ –±—ã–ª–∏ —Å–∫–∞—á–∞–Ω—ã
            if media_markers:
                entry["media_markers"] = media_markers
                logger.info(f"   ‚úÖ Stored {len(media_markers)} media_markers in entry for day {day}")
                for marker_id in media_markers.keys():
                    logger.info(f"   üìé     - {marker_id}")
            else:
                logger.warning(f"   ‚ö†Ô∏è No media_markers for day {day} (drive_links found: {len(drive_links)})")
            compiled[str(day)] = entry
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            total_blocks += len(lesson_posts)
            total_media_files += len(media_markers)

        return compiled, media_downloaded, total_blocks, total_media_files

    def _build_drive_client(self):
        # Lazy import to avoid hard dependency if feature disabled
        from google.oauth2 import service_account
        from googleapiclient.discovery import build

        raw_json = (Config.GOOGLE_SERVICE_ACCOUNT_JSON or "").strip()
        if not raw_json:
            b64 = (Config.GOOGLE_SERVICE_ACCOUNT_JSON_B64 or "").strip()
            raw_json = base64.b64decode(b64.encode("utf-8")).decode("utf-8")

        info = json.loads(raw_json)
        creds = service_account.Credentials.from_service_account_info(
            info,
            scopes=["https://www.googleapis.com/auth/drive.readonly"],
        )
        return build("drive", "v3", credentials=creds, cache_discovery=False)

    @staticmethod
    def _parse_day_from_name(name: str) -> Optional[int]:
        """
        Accepts: day_1, day-01, 01, 1, –î–µ–Ω—å 1, etc.
        """
        name = (name or "").strip()
        m = re.search(r"(?i)(?:day[_-]?|–¥–µ–Ω—å\s*)?(\d{1,2})\b", name)
        if not m:
            return None
        day = int(m.group(1))
        if 0 <= day <= 30:
            return day
        return None

    def _list_children(self, drive, parent_id: str) -> List[Dict[str, Any]]:
        files: List[Dict[str, Any]] = []
        page_token = None
        while True:
            resp = drive.files().list(
                q=f"'{parent_id}' in parents and trashed=false",
                fields="nextPageToken, files(id,name,mimeType,modifiedTime,size)",
                pageToken=page_token,
                pageSize=1000,
            ).execute()
            files.extend(resp.get("files", []))
            page_token = resp.get("nextPageToken")
            if not page_token:
                break
        return files

    def _download_text_file(self, drive, file_id: str, mime_type: str) -> str:
        """
        For Google Docs: export to text/plain.
        For other files: download bytes and decode utf-8.
        """
        if mime_type == GOOGLE_DOC_MIME:
            data = drive.files().export(fileId=file_id, mimeType="text/plain").execute()
            # google api returns bytes in this call
            if isinstance(data, str):
                return data
            return data.decode("utf-8", errors="replace")

        request = drive.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        from googleapiclient.http import MediaIoBaseDownload

        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            _, done = downloader.next_chunk()
        return fh.getvalue().decode("utf-8", errors="replace")

    def _download_binary_file(self, drive, file_id: str, dest_path: Path) -> None:
        dest_path.parent.mkdir(parents=True, exist_ok=True)

        request = drive.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        from googleapiclient.http import MediaIoBaseDownload

        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            _, done = downloader.next_chunk()

        # Atomic write
        tmp = dest_path.with_suffix(dest_path.suffix + ".tmp")
        with open(tmp, "wb") as f:
            f.write(fh.getvalue())
        os.replace(tmp, dest_path)

    @staticmethod
    def _parse_rfc3339(value: str) -> Optional[datetime]:
        if not value:
            return None
        try:
            # Google Drive returns RFC3339, often with "Z"
            v = value.replace("Z", "+00:00")
            return datetime.fromisoformat(v)
        except Exception:
            return None

    @classmethod
    def _should_skip_download(cls, dest_path: Path, remote_size: Optional[str], remote_mtime: Optional[str]) -> bool:
        if not dest_path.exists():
            return False
        try:
            if remote_size:
                size = int(remote_size)
                if size > 0 and dest_path.stat().st_size != size:
                    return False
            remote_dt = cls._parse_rfc3339(remote_mtime)
            if remote_dt:
                local_dt = datetime.fromtimestamp(dest_path.stat().st_mtime, tz=timezone.utc)
                if local_dt >= remote_dt:
                    return True
            # If size matches and no mtime, treat as cached
            if remote_size:
                return True
        except Exception:
            return False
        return False

    @staticmethod
    def _sanitize_telegram_html(text: str) -> Tuple[str, List[str]]:
        """
        Telegram HTML is a strict subset. Editors will type tags directly in Google Docs.
        This sanitizer keeps only safe Telegram tags and escapes everything else.

        Allowed tags (Telegram HTML): b/strong, i/em, u/ins, s/strike/del, code, pre,
        a (href only), tg-spoiler, blockquote.
        Also supports span class="tg-spoiler" (Telegram spoiler).
        
        –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–ø—Ä–æ–±–µ–ª—ã, –æ—Ç—Å—Ç—É–ø—ã, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, —ç–º–æ–¥–∑–∏)
        """
        warnings: List[str] = []

        allowed = {
            "b", "strong",
            "i", "em",
            "u", "ins",
            "s", "strike", "del",
            "code", "pre",
            "a",
            "tg-spoiler",
            "blockquote",
            "span",
        }

        class _Sanitizer(HTMLParser):
            def __init__(self):
                super().__init__(convert_charrefs=False)
                self.out: List[str] = []

            def handle_data(self, data: str) -> None:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –µ—Å—Ç—å (–≤–∫–ª—é—á–∞—è –ø—Ä–æ–±–µ–ª—ã, –æ—Ç—Å—Ç—É–ø—ã, —ç–º–æ–¥–∑–∏)
                self.out.append(data)

            def handle_entityref(self, name: str) -> None:
                self.out.append(f"&{name};")

            def handle_charref(self, name: str) -> None:
                self.out.append(f"&#{name};")

            def handle_starttag(self, tag: str, attrs: List[Tuple[str, Optional[str]]]) -> None:
                tag_l = (tag or "").lower()
                if tag_l not in allowed:
                    warnings.append(f"removed tag <{tag}>")
                    self.out.append(html.escape(f"<{tag}>"))
                    return

                if tag_l == "a":
                    href = ""
                    for k, v in attrs:
                        if (k or "").lower() == "href" and v:
                            href = v
                            break
                    if not href:
                        warnings.append("a tag without href removed")
                        self.out.append(html.escape("<a>"))
                        return
                    safe_href = href.replace("\"", "&quot;")
                    self.out.append(f"<a href=\"{safe_href}\">")
                    return

                if tag_l == "span":
                    # allow only spoiler span
                    cls = ""
                    for k, v in attrs:
                        if (k or "").lower() == "class" and v:
                            cls = v
                            break
                    if cls != "tg-spoiler":
                        warnings.append("span tag without class=tg-spoiler removed")
                        self.out.append(html.escape("<span>"))
                        return
                    self.out.append("<span class=\"tg-spoiler\">")
                    return

                # other allowed tags with no attrs
                self.out.append(f"<{tag_l}>")

            def handle_endtag(self, tag: str) -> None:
                tag_l = (tag or "").lower()
                if tag_l not in allowed:
                    self.out.append(html.escape(f"</{tag}>"))
                    return
                if tag_l == "strong":
                    tag_l = "b"
                if tag_l == "em":
                    tag_l = "i"
                if tag_l == "ins":
                    tag_l = "u"
                if tag_l == "strike":
                    tag_l = "s"
                self.out.append(f"</{tag_l}>")

        parser = _Sanitizer()
        try:
            parser.feed(text or "")
            parser.close()
        except Exception as e:
            # If parsing fails, escape everything to avoid Telegram parse errors
            return html.escape(text or ""), [f"html parse error: {e}"]

        sanitized = "".join(parser.out)

        # If someone typed "1 < 2" it could be interpreted as a tag start; best-effort fix:
        # escape any remaining "<" that doesn't look like a tag start.
        sanitized = re.sub(r"<(?!/?(?:b|strong|i|em|u|ins|s|strike|del|code|pre|a|tg-spoiler|blockquote|span)\b)", "&lt;", sanitized)

        return sanitized, warnings

    @staticmethod
    def _pick_named(files: List[Dict[str, Any]], base: str) -> Optional[Dict[str, Any]]:
        """
        Pick file whose name starts with base (lesson/task/meta) ignoring extension and case.
        Prefer: .html, .txt, Google Doc.
        """
        base_l = base.lower()
        candidates = []
        for f in files:
            name = (f.get("name") or "").strip()
            if not name:
                continue
            stem = name.rsplit(".", 1)[0].lower()
            if stem == base_l or stem.startswith(base_l):
                candidates.append(f)
        if not candidates:
            return None
        # prefer html, then txt, then doc
        def score(f: Dict[str, Any]) -> int:
            n = (f.get("name") or "").lower()
            mt = f.get("mimeType")
            if n.endswith(".html"):
                return 0
            if n.endswith(".txt"):
                return 1
            if mt == GOOGLE_DOC_MIME:
                return 2
            return 3
        candidates.sort(key=score)
        return candidates[0]

    def _target_lessons_path(self) -> Path:
        # Put lessons.json next to the DB, so Railway Volume persists it (/app/data)
        db_path = Path(Config.DATABASE_PATH)
        return db_path.parent / "lessons.json"

    def _backup_file_if_exists(self, target: Path) -> Optional[Path]:
        """
        Create a timestamped backup copy next to the target, if it exists.
        Returns backup path if created.
        """
        try:
            if not target.exists() or not target.is_file():
                return None
            backups_dir = target.parent / "content_backups"
            backups_dir.mkdir(parents=True, exist_ok=True)
            ts = datetime.utcnow().strftime("%Y%m%d-%H%M%S")
            backup_path = backups_dir / f"{target.stem}.{ts}{target.suffix}"
            shutil.copy2(target, backup_path)
            logger.info(f"‚úÖ Backup created: {backup_path}")
            return backup_path
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to backup {target}: {e}")
            return None
    
    def get_latest_backup(self) -> Optional[Path]:
        """Get the most recent backup file path."""
        target = self._target_lessons_path()
        backups_dir = target.parent / "content_backups"
        if not backups_dir.exists():
            return None
        backups = sorted(backups_dir.glob(f"{target.stem}.*{target.suffix}"), reverse=True)
        return backups[0] if backups else None
    
    def get_all_backups(self) -> List[Tuple[Path, datetime]]:
        """Get all backup files with their timestamps."""
        target = self._target_lessons_path()
        backups_dir = target.parent / "content_backups"
        if not backups_dir.exists():
            return []
        backups = []
        for backup_path in backups_dir.glob(f"{target.stem}.*{target.suffix}"):
            try:
                # Extract timestamp from filename: lessons.YYYYMMDD-HHMMSS.json
                parts = backup_path.stem.split(".")
                if len(parts) >= 2:
                    ts_str = parts[-1]
                    ts = datetime.strptime(ts_str, "%Y%m%d-%H%M%S")
                    backups.append((backup_path, ts))
            except Exception:
                # If timestamp parsing fails, use file mtime
                ts = datetime.fromtimestamp(backup_path.stat().st_mtime)
                backups.append((backup_path, ts))
        return sorted(backups, key=lambda x: x[1], reverse=True)
    
    def restore_from_backup(self, backup_path: Optional[Path] = None) -> bool:
        """
        Restore lessons.json from a backup.
        If backup_path is None, uses the latest backup.
        Returns True if successful.
        """
        try:
            if backup_path is None:
                backup_path = self.get_latest_backup()
            if not backup_path or not backup_path.exists():
                logger.error("No backup found to restore from")
                return False
            
            target = self._target_lessons_path()
            target.parent.mkdir(parents=True, exist_ok=True)
            
            # Create a backup of current file before restoring
            if target.exists():
                self._backup_file_if_exists(target)
            
            shutil.copy2(backup_path, target)
            logger.info(f"‚úÖ Restored lessons.json from backup: {backup_path}")
            return True
        except Exception as e:
            logger.error(f"‚ùå Failed to restore from backup: {e}", exc_info=True)
            return False

    def clean_media_files(self) -> int:
        """
        –£–¥–∞–ª—è–µ—Ç –≤—Å–µ –º–µ–¥–∏–∞—Ñ–∞–π–ª—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ content_media.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.
        """
        project_root = Path.cwd()
        media_root = (project_root / self.media_dir).resolve()
        
        if not media_root.exists():
            logger.info(f"üìÅ Media directory does not exist: {media_root}")
            return 0
        
        deleted_count = 0
        try:
            # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (—á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤–æ –≤—Ä–µ–º—è –∏—Ç–µ—Ä–∞—Ü–∏–∏)
            files_to_delete = []
            dirs_to_delete = []
            
            for item in media_root.iterdir():
                if item.is_file():
                    files_to_delete.append(item)
                elif item.is_dir():
                    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
                    for subitem in item.rglob("*"):
                        if subitem.is_file():
                            files_to_delete.append(subitem)
                    dirs_to_delete.append(item)
            
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
            for file_path in files_to_delete:
                try:
                    file_path.unlink()
                    deleted_count += 1
                    logger.debug(f"   üóëÔ∏è Deleted file: {file_path.relative_to(media_root)}")
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è Could not delete file {file_path}: {e}")
            
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (shutil.rmtree —É–¥–∞–ª—è–µ—Ç —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ, –≤–∫–ª—é—á–∞—è –≤—Å–µ —Ñ–∞–π–ª—ã)
            for dir_path in dirs_to_delete:
                try:
                    shutil.rmtree(dir_path)
                    logger.debug(f"   üóëÔ∏è Deleted directory: {dir_path.relative_to(media_root)}")
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è Could not delete directory {dir_path}: {e}")
                    # –ü—Ä–æ–±—É–µ–º —É–¥–∞–ª–∏—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ñ–∞–π–ª—ã –≤—Ä—É—á–Ω—É—é
                    for remaining in dir_path.rglob("*"):
                        if remaining.is_file():
                            try:
                                remaining.unlink()
                                deleted_count += 1
                            except Exception:
                                pass
                    # –ü—Ä–æ–±—É–µ–º —É–¥–∞–ª–∏—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å–Ω–æ–≤–∞
                    try:
                        dir_path.rmdir()
                    except Exception:
                        pass
            
            logger.info(f"‚úÖ Cleaned {deleted_count} media files from {media_root}")
            return deleted_count
        except Exception as e:
            logger.error(f"‚ùå Error cleaning media files: {e}", exc_info=True)
            raise

    def sync_now(self, clean_media: bool = False) -> SyncResult:
        ok, reason = self._admin_ready()
        if not ok:
            raise RuntimeError(f"Drive content sync not ready: {reason}")

        # –û—á–∏—â–∞–µ–º –º–µ–¥–∏–∞—Ñ–∞–π–ª—ã –ø–µ—Ä–µ–¥ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–µ–π, –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
        if clean_media:
            logger.info("üßπ Cleaning media files before sync...")
            deleted_count = self.clean_media_files()
            logger.info(f"‚úÖ Cleaned {deleted_count} media files")

        drive = self._build_drive_client()
        warnings: List[str] = []

        # Single-doc mode
        if (Config.DRIVE_MASTER_DOC_ID or "").strip():
            compiled, media_downloaded, total_blocks, total_media_files = self._sync_from_master_doc(drive, warnings)
            
            # Basic validation: ensure each lesson has text
            for k, v in compiled.items():
                text = v.get("text", "")
                if isinstance(text, list):
                    if not text or all(not (block or "").strip() for block in text):
                        warnings.append(f"day {k}: empty lesson text (all blocks are empty)")
                elif not (text or "").strip():
                    warnings.append(f"day {k}: empty lesson text")
            
            target = self._target_lessons_path()
            target.parent.mkdir(parents=True, exist_ok=True)
            self._backup_file_if_exists(target)
            tmp = target.with_suffix(".json.tmp")
            with open(tmp, "w", encoding="utf-8") as f:
                json.dump(compiled, f, ensure_ascii=False, indent=2)
            os.replace(tmp, target)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –±–ª–æ–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
            total_saved_blocks = 0
            total_saved_chars = 0
            empty_blocks_found = 0
            for day_key, entry in compiled.items():
                text = entry.get("text", "")
                if isinstance(text, list):
                    total_saved_blocks += len(text)
                    for block in text:
                        if block and block.strip():
                            total_saved_chars += len(block)
                        else:
                            empty_blocks_found += 1
                elif text:
                    total_saved_blocks += 1
                    if text.strip():
                        total_saved_chars += len(text)
                    else:
                        empty_blocks_found += 1
            
            logger.info(f"‚úÖ Drive master-doc sync wrote {len(compiled)} lessons to {target}")
            logger.info(f"   üì¶ Total blocks saved: {total_saved_blocks} (expected: {total_blocks})")
            logger.info(f"   üìù Total characters saved: {total_saved_chars}")
            if total_saved_blocks != total_blocks:
                logger.warning(f"   ‚ö†Ô∏è Block count mismatch! Saved: {total_saved_blocks}, Expected: {total_blocks}")
            if empty_blocks_found > 0:
                logger.warning(f"   ‚ö†Ô∏è Found {empty_blocks_found} empty blocks in saved data!")
            return SyncResult(
                days_synced=len(compiled),
                lessons_path=str(target),
                media_files_downloaded=media_downloaded,
                total_blocks=total_blocks,
                total_media_files=total_media_files,
                warnings=warnings,
            )

        root_children = self._list_children(drive, self.root_folder_id)
        day_folders: List[Tuple[int, Dict[str, Any]]] = []
        for f in root_children:
            if f.get("mimeType") != FOLDER_MIME:
                continue
            day = self._parse_day_from_name(f.get("name", ""))
            if day is None:
                continue
            day_folders.append((day, f))
        day_folders.sort(key=lambda x: x[0])

        if not day_folders:
            raise RuntimeError("No day folders found in Drive root (expected day_00..day_30)")

        project_root = Path.cwd()
        media_root = (project_root / self.media_dir).resolve()

        compiled: Dict[str, Any] = {}
        media_downloaded = 0
        total_blocks = 0
        total_media_files = 0

        for day, folder in day_folders:
            folder_id = folder["id"]
            children = self._list_children(drive, folder_id)

            # If there's a "media" subfolder, include its contents too.
            media_folder = None
            for c in children:
                if c.get("mimeType") == FOLDER_MIME and (c.get("name") or "").lower() == "media":
                    media_folder = c
                    break
            media_children = self._list_children(drive, media_folder["id"]) if media_folder else []

            lesson_file = self._pick_named(children, "lesson")
            task_file = self._pick_named(children, "task")
            meta_file = self._pick_named(children, "meta")

            if not lesson_file:
                warnings.append(f"day {day}: missing lesson file")
                continue

            lesson_text = self._download_text_file(drive, lesson_file["id"], lesson_file.get("mimeType", ""))
            task_text = ""
            if task_file:
                task_text = self._download_text_file(drive, task_file["id"], task_file.get("mimeType", ""))

            # Telegram HTML sanitizer (editors type tags directly in Google Docs)
            lesson_text, w1 = self._sanitize_telegram_html(lesson_text or "")
            if w1:
                warnings.extend([f"day {day}: {w}" for w in w1])
            if task_text:
                task_text, w2 = self._sanitize_telegram_html(task_text or "")
                if w2:
                    warnings.extend([f"day {day}: {w}" for w in w2])
            
            # Process Drive-linked media referenced in the text/task (same as in _sync_from_master_doc)
            # Replace links with markers and download files
            media_markers: Dict[str, Dict[str, Any]] = {}  # marker_id -> media_info
            
            # Find all Drive links in lesson and task text
            combined_text = (lesson_text or "") + "\n" + (task_text or "")
            drive_links = self._find_drive_links_with_positions(combined_text)
            
            logger.info(f"   üìé Day {day}: Found {len(drive_links)} Drive links in text")
            logger.info(f"   üìé Day {day}: Text lengths - lesson: {len(lesson_text or '')}, task: {len(task_text or '')}")
            if drive_links:
                for idx, link in enumerate(drive_links, 1):
                    logger.info(f"   üìé   [{idx}/{len(drive_links)}] Link: {link['url'][:80]}... (file_id: {link['file_id']}, is_folder: {link.get('is_folder', False)}, position: {link.get('start', '?')}-{link.get('end', '?')})")
            else:
                logger.warning(f"   ‚ö†Ô∏è Day {day}: No Drive links found in text! This may indicate a problem with link detection.")
            
            # Process links in reverse order to preserve positions when replacing
            drive_links.sort(key=lambda x: x["start"], reverse=True)
            
            processed_links = 0
            skipped_links = 0
            error_links = 0
            
            for link_info in drive_links:
                fid = link_info["file_id"]
                link_url = link_info["url"]
                is_folder = link_info.get("is_folder", False)
                
                try:
                    if is_folder:
                        # Handle folder: get all files in folder and process each as media
                        folder_id = link_info.get("folder_id") or fid
                        logger.info(f"   üìÅ Processing Drive folder: {link_url[:60]}... (folder_id: {folder_id})")
                        
                        folder_files = self._list_children(drive, folder_id)
                        logger.info(f"   üìÅ   Found {len(folder_files)} items in folder")
                        
                        if not folder_files:
                            logger.warning(f"   ‚ö†Ô∏è Folder {folder_id} is empty or inaccessible")
                            skipped_links += 1
                            continue
                        
                        folder_markers = []
                        for folder_file in folder_files:
                            file_id = folder_file.get("id")
                            file_name = folder_file.get("name", f"file_{file_id}").strip()
                            file_mime = (folder_file.get("mimeType") or "").lower()
                            
                            if file_mime.startswith("image/"):
                                media_type = "photo"
                            elif file_mime.startswith("video/"):
                                media_type = "video"
                            else:
                                continue
                            
                            safe_name = re.sub(r"[^a-zA-Z0-9._-]+", "_", file_name)
                            dest = media_root / f"day_{day:02d}" / safe_name
                            dest.parent.mkdir(parents=True, exist_ok=True)
                            
                            should_skip = self._should_skip_download(dest, folder_file.get("size"), folder_file.get("modifiedTime"))
                            if not should_skip or not dest.exists():
                                self._download_binary_file(drive, file_id, dest)
                                media_downloaded += 1
                            
                        processed_links += 1
                        rel_path = str(dest.relative_to(project_root)).replace("\\", "/")
                        
                        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª –ª–∏ —É–∂–µ —Å–æ–∑–¥–∞–Ω –º–∞—Ä–∫–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ file_id
                        marker_id = None
                        for existing_marker_id, existing_info in media_markers.items():
                            if existing_info.get("file_id") == file_id and existing_info.get("path") == rel_path:
                                marker_id = existing_marker_id
                                logger.info(f"   ‚ôªÔ∏è Reusing existing marker: [{marker_id}] for file {file_name} (file_id: {file_id})")
                                break
                        
                        # –ï—Å–ª–∏ –º–∞—Ä–∫–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
                        if not marker_id:
                            marker_id = f"MEDIA_{file_id}_{len(media_markers)}"
                            media_markers[marker_id] = {
                                "type": media_type,
                                "path": rel_path,
                                "file_id": file_id,
                                "name": file_name
                            }
                            logger.info(f"   ‚úÖ Created new media marker: [{marker_id}] for file {file_name} (path: {rel_path})")
                        
                        folder_markers.append(marker_id)
                        
                        if folder_markers:
                            markers_text = "\n".join([f"[{m}]" for m in folder_markers])
                            if link_url in lesson_text:
                                lesson_text = lesson_text.replace(link_url, markers_text)
                            if link_url in task_text:
                                task_text = task_text.replace(link_url, markers_text)
                    else:
                        # Handle single file
                        logger.info(f"   üìé Processing Drive link: {link_url[:60]}... (file_id: {fid})")
                        meta = drive.files().get(fileId=fid, fields="id,name,mimeType,modifiedTime,size").execute()
                        mt = (meta.get("mimeType") or "").lower()
                        name = (meta.get("name") or f"file_{fid}").strip()
                        
                        if mt.startswith("image/"):
                            media_type = "photo"
                        elif mt.startswith("video/"):
                            media_type = "video"
                        else:
                            skipped_links += 1
                            continue
                        
                        safe_name = re.sub(r"[^a-zA-Z0-9._-]+", "_", name)
                        dest = media_root / f"day_{day:02d}" / safe_name
                        dest.parent.mkdir(parents=True, exist_ok=True)
                        
                        should_skip = self._should_skip_download(dest, meta.get("size"), meta.get("modifiedTime"))
                        if not should_skip or not dest.exists():
                            self._download_binary_file(drive, fid, dest)
                            media_downloaded += 1
                        
                        processed_links += 1
                        rel_path = str(dest.relative_to(project_root)).replace("\\", "/")
                        
                        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª –ª–∏ —É–∂–µ —Å–æ–∑–¥–∞–Ω –º–∞—Ä–∫–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ file_id
                        marker_id = None
                        for existing_marker_id, existing_info in media_markers.items():
                            if existing_info.get("file_id") == fid and existing_info.get("path") == rel_path:
                                marker_id = existing_marker_id
                                logger.info(f"   ‚ôªÔ∏è Reusing existing marker: [{marker_id}] for file {name} (file_id: {fid})")
                                break
                        
                        # –ï—Å–ª–∏ –º–∞—Ä–∫–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
                        if not marker_id:
                            marker_id = f"MEDIA_{fid}_{len(media_markers)}"
                            media_markers[marker_id] = {
                                "type": media_type,
                                "path": rel_path,
                                "file_id": fid,
                                "name": name
                            }
                            logger.info(f"   ‚úÖ Created new media marker: [{marker_id}] for file {name} (path: {rel_path})")
                        
                        marker_placeholder = f"[{marker_id}]"
                        if link_url in lesson_text:
                            lesson_text = lesson_text.replace(link_url, marker_placeholder)
                        if link_url in task_text:
                            task_text = task_text.replace(link_url, marker_placeholder)
                except Exception as e:
                    error_links += 1
                    error_msg = f"day {day}: failed to download linked media ({fid}): {e}"
                    logger.error(f"   ‚ùå {error_msg}")
                    warnings.append(error_msg)
            
            if drive_links:
                logger.info(f"   üìé Day {day} summary: {processed_links} processed, {skipped_links} skipped, {error_links} errors, {media_downloaded} downloaded")

            meta: Dict[str, Any] = {}
            if meta_file and (meta_file.get("name") or "").lower().endswith(".json"):
                try:
                    meta_raw = self._download_text_file(drive, meta_file["id"], meta_file.get("mimeType", ""))
                    meta = json.loads(meta_raw)
                except Exception as e:
                    warnings.append(f"day {day}: meta.json invalid ({e})")

            # Collect media files (images/videos) from day folder + media subfolder
            media_items: List[Dict[str, Any]] = []
            media_sources = [c for c in (children + media_children) if c.get("mimeType") not in (FOLDER_MIME, GOOGLE_DOC_MIME)]

            for m in media_sources:
                name = (m.get("name") or "").strip()
                mt = (m.get("mimeType") or "").lower()
                if not name or not mt:
                    continue
                if mt.startswith("image/"):
                    media_type = "photo"
                elif mt.startswith("video/"):
                    media_type = "video"
                else:
                    continue

                safe_name = re.sub(r"[^a-zA-Z0-9._-]+", "_", name)
                dest = media_root / f"day_{day:02d}" / safe_name
                try:
                    if not self._should_skip_download(dest, m.get("size"), m.get("modifiedTime")):
                        self._download_binary_file(drive, m["id"], dest)
                        media_downloaded += 1
                    # Store path relative to project root, because CourseBot resolves it that way
                    rel_path = str(dest.relative_to(project_root)).replace("\\", "/")
                    media_items.append({"type": media_type, "path": rel_path})
                except Exception as e:
                    warnings.append(f"day {day}: failed to download media {name} ({e})")

            title = (meta.get("title") or "").strip() if isinstance(meta, dict) else ""
            if not title:
                title = f"–î–µ–Ω—å {day}"

            # Split lesson into posts by square brackets
            lesson_text_clean = (lesson_text or "").rstrip()
            lesson_posts = DriveContentSync._split_lesson_into_posts(lesson_text_clean)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –±–ª–æ–∫–∞—Ö –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            logger.info(f"   üì¶ Day {day}: Split into {len(lesson_posts)} blocks")
            for i, post in enumerate(lesson_posts, 1):
                post_preview = post[:100].replace('\n', ' ') if post else "(empty)"
                logger.debug(f"   üì¶   Block {i}/{len(lesson_posts)}: {len(post)} chars, preview: {post_preview}...")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –±–ª–æ–∫–æ–≤: –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ—Ç –ø—É—Å—Ç—ã—Ö –±–ª–æ–∫–æ–≤
            valid_lesson_posts = [post for post in lesson_posts if post and post.strip()]
            if len(valid_lesson_posts) != len(lesson_posts):
                empty_count = len(lesson_posts) - len(valid_lesson_posts)
                logger.warning(f"   ‚ö†Ô∏è Day {day}: Found {empty_count} empty blocks, removing them")
                lesson_posts = valid_lesson_posts
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è
            original_length = len(lesson_text_clean)
            saved_length = sum(len(post) for post in lesson_posts)
            if saved_length < original_length * 0.95:  # –î–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–±–æ–ª—å—à—É—é –ø–æ—Ç–µ—Ä—é –ø—Ä–∏ rstrip()
                logger.warning(f"   ‚ö†Ô∏è Day {day}: Possible text loss detected! Original: {original_length} chars, Saved: {saved_length} chars")
            else:
                logger.debug(f"   ‚úÖ Day {day}: Text integrity check passed (original: {original_length}, saved: {saved_length} chars)")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç: —Å–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤, –µ—Å–ª–∏ –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ, –∏–Ω–∞—á–µ —Å—Ç—Ä–æ–∫–∞
            text_to_save = lesson_posts if len(lesson_posts) > 1 else (lesson_posts[0] if lesson_posts else "")
            if isinstance(text_to_save, list):
                logger.info(f"   ‚úÖ Day {day}: Saving {len(text_to_save)} blocks as list (total {saved_length} chars)")
            else:
                logger.info(f"   ‚úÖ Day {day}: Saving single block as string ({len(text_to_save) if text_to_save else 0} chars)")
            
            entry: Dict[str, Any] = {
                "day_number": day,
                "title": title,
                "text": text_to_save,
                "task": (task_text or "").rstrip(),
            }
            if media_items:
                entry["media"] = media_items
            # CRITICAL: Store media markers for inline insertion
            if media_markers:
                entry["media_markers"] = media_markers
                logger.info(f"   ‚úÖ Stored {len(media_markers)} media_markers in entry for day {day}")
            if isinstance(meta, dict) and "silent" in meta:
                entry["silent"] = bool(meta.get("silent"))

            compiled[str(day)] = entry
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            total_blocks += len(lesson_posts)
            total_media_files += len(media_markers)

        if not compiled:
            raise RuntimeError("No lessons compiled (check Drive folder contents)")

        # Basic validation: ensure each lesson has text
        for k, v in compiled.items():
            text = v.get("text", "")
            if isinstance(text, list):
                if not text or all(not (block or "").strip() for block in text):
                    warnings.append(f"day {k}: empty lesson text (all blocks are empty)")
            elif not (text or "").strip():
                warnings.append(f"day {k}: empty lesson text")

        target = self._target_lessons_path()
        target.parent.mkdir(parents=True, exist_ok=True)
        self._backup_file_if_exists(target)
        tmp = target.with_suffix(".json.tmp")
        with open(tmp, "w", encoding="utf-8") as f:
            json.dump(compiled, f, ensure_ascii=False, indent=2)
        os.replace(tmp, target)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –±–ª–æ–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
        total_saved_blocks = 0
        total_saved_chars = 0
        empty_blocks_found = 0
        for day_key, entry in compiled.items():
            text = entry.get("text", "")
            if isinstance(text, list):
                total_saved_blocks += len(text)
                for block in text:
                    if block and block.strip():
                        total_saved_chars += len(block)
                    else:
                        empty_blocks_found += 1
            elif text:
                total_saved_blocks += 1
                if text.strip():
                    total_saved_chars += len(text)
                else:
                    empty_blocks_found += 1
        
        logger.info(f"‚úÖ Drive sync wrote {len(compiled)} lessons to {target}")
        logger.info(f"   üì¶ Total blocks saved: {total_saved_blocks} (expected: {total_blocks})")
        logger.info(f"   üìù Total characters saved: {total_saved_chars}")
        if total_saved_blocks != total_blocks:
            logger.warning(f"   ‚ö†Ô∏è Block count mismatch! Saved: {total_saved_blocks}, Expected: {total_blocks}")
        if empty_blocks_found > 0:
            logger.warning(f"   ‚ö†Ô∏è Found {empty_blocks_found} empty blocks in saved data!")
        return SyncResult(
            days_synced=len(compiled),
            lessons_path=str(target),
            media_files_downloaded=media_downloaded,
            total_blocks=total_blocks,
            total_media_files=total_media_files,
            warnings=warnings,
        )
